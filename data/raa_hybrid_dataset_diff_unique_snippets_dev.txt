BSIF have been used for several applications including biometrics from iris images [17,12,24]. In this work, a gender classification algorithm using normalised nir iris images is proposed. It uses a similar pipeline than iris recognition systems. The iris is segmented and occlusions are masked. BSIF can be sensitive to image boundaries and the occlusion mask creating artificial texture which may mislead gender classification results.
Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100\u00d7 more data. We opensource our pretrained models and code 1 .
The cost of vision-and-language pre-training has become increasingly prohibitive due to end-toend training of large-scale models. This paper proposes blip-2, a generic and efficient pretraining strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. blip-2 bridges the modality gap with a lightweight Querying Transformer, which is pretrained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various visionlanguage tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.
The answer selection problem can be formulated as follows: Given a question q and an answer candidate pool {a 1 , a 2 , \u2022 \u2022 \u2022 , a s } for this question, we aim to search for the best answer candidate a k , where 1 \u2264 k \u2264 s. An answer is a token sequence with an arbitrary length, and a question can correspond to multiple ground-truth answers. In testing, the candidate answers for a question may not be observed in the training phase. Answer selection is one of the essential components in typical question answering (QA) systems. It is also a stand-alone task with applications in knowledge base construction and information extraction.
We present xphonebert, the first multilingual model pretrained to learn phoneme representations for the downstream text-to-speech (TTS) task. Our xphonebert has the same model architecture as BERT-base, trained using the RoBERTa pre-training approach on 330M phoneme-level sentences from nearly 100 languages and locales. Experimental results show that employing xphonebert as an input phoneme encoder significantly boosts the performance of a strong neural TTS model in terms of naturalness and prosody and also helps produce fairly high-quality speech with limited training data. We publicly release our pre-trained xphonebert with the hope that it would facilitate future research and downstream TTS applications for multiple languages.
Since Google Cloud did not have APIs for all tasks we study in this paper, we extrapolated the costs of the entity analysis and sentiment analysis APIs for natural language inference (MNLI) and reading comprehension (SQuAD, BoolQ). We believe this is a reasonable estimate since every model studied in this paper is a single layer in addition to BERT-large (thereby needing a similar number of FLOPs for similar input lengths).
Tram\u00e8r et al. (2016)racy using WIKI queries on MNLI and SQuAD with mismatched BERT architectures between the victim and attacker.Note the trend: (large, large) > (base, large) > (base, base) > (large, base) where the (\u2022, \u2022) refers to (victim, attacker) pretraining.Next, we experiment with an alternative non-BERT pretrained language model as the attacker architecture.We use XLNet-large(Yang et al., 2019), which has been shown to outperform BERT-large in a large variety of downstream NLP tasks.In Table5, we compare XLNet-large and bert-large attacker architectures keeping a fixed BERT-large victim architecture.Note the superior performance of xlnet-large attacker models on SQuAD compared to BERT-large in both RANDOM and WIKI attack settings, despite seeing a mismatched victim's (BERT-large) outputs during training.Our experiments are reminiscent of similar discussion inTram\u00e8r et al. (2016)on Occam Learning, or appropriate alignment of victim-attacker architectures. Overall, the results suggest that attackers can maximize their accuracy by fine-tuning more powerful language models, and that matching architectures is a secondary concern.
To further improve training efficiency, we reduced the amount of activations that are recomputed during the backward pass with checkpointing. More precisely, we save the activations that are expensive to compute, such as the outputs of linear layers. This is achieved by manually implementing the backward function for the transformer layers, instead of relying on the PyTorch autograd. reduce the memory usage of the model by using model and sequence parallelism, as described by Korthikanti et al. (2022). Moreover, we also overlap the computation of activations and the communication between GPUs over the network (due to all_reduce operations) as much as possible.
To fill the gap, we train the first large-scale multilingual language model for phoneme representations, using a pre-training corpus of 330M phonemic description sentences from nearly 100 languages and locales. Our model is trained based on the RoBERTa pre-training approach [11], using the bert-base model configuration [10]. We conduct experiments on the downstream TTS task, directly employing our model as an input phoneme encoder of the strong model VITS [9]. Experimental results show that our model helps boost the performance of VITS, obtaining more natural prosody than the original VITS without pre-training and also producing fairly high-quality synthesized speech with limited training data. We summarize our contribution as follows:

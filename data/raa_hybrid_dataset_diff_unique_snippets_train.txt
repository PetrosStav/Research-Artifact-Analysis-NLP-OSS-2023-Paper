xphonebert has the same model architecture as BERT-base [10]-a multi-layer bidirectional Transformer encoder [20]-in which the number of Transformer blocks, the hidden size and the number of self-attention heads are 12, 768 and 12, respectively. To pre-train xphonebert, we use the masked language modeling objective [10] and follow the RoBERTa pre-training approach [11] which robustly optimizes BERT for better performance, i.e. using a dynamic masking strategy and without the next sentence prediction objective. Given the popularity of BERT and RoBERTa, we do not further detail about the architecture here. See [10,11] for more information.
To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources.
To convert sentences into their phonemic description, we employ the grapheme-to-phoneme conversion toolkit Char-siuG2P [21]. The pre-trained charsiug2p is a strong multilingual Transformer-based model that generates the pronunciation of a word given its orthographic form and ISO 639-3 language code pair. Following the recommendation from [21], if the input word is in the charsiug2p toolkit's pronunciation dictionary of the target language/locale, we employ the pronunciation dictionary to generate the word's phonemic description. Otherwise, if the word is out of the vocabulary, we employ the pre-trained charsiug2p model to generate its phonemic description.
For English, we use the benchmark dataset ljspeech [32] consisting of 13,100 audio clips of a single speaker with a total duration of about 24 hours (here, each clip is also provided with a gold-standard text transcription). Following [9], the dataset is split into training, validation and test sets of 12,500, 100 and 500 clip samples, respectively. For Vietnamese, we randomly sample 12,300 different medium-length sentences from the PhoBERT pre-training news data [33]. We hire a professional speaker to read each sentence in a studio and record the corresponding audio, resulting in a total duration of about 18 hours for 12,300 high-quality audio clips. We split our Vietnamese TTS dataset into training, validation and test sets of 12,000, 100 and 200 clips, respectively.
We show the efficacy of HQ-SAM in a suite of 9 diverse segmentation datasets across different downstream tasks, where 7 out of them are evaluated in a zero-shot transfer protocol.
The two extensions introduced previously are combined in a simple manner. First, the biLSTM hidden vectors of answers h a (t) are multiplied by s a,q (t), which is computed from the question average pooling vectors o q , and updated to h a (t), illustrated in Eq. 9-11. Then, the original question and updated answer hidden vectors serve as inputs of CNN structure respectively, such that the question context can be used to evaluate the softmax weights of the input of CNN. From the experiments, we observe that the two extensions vary on their contributions on the performance improvement according to different datasets. However, qa-lstm/cnn with attention can outperform the baselines on both datasets.
In our controlled setting (Table 2), our extracted models are surprisingly accurate on the original development sets of all tasks, even when trained with nonsensical inputs (RANDOM) that do not match the original data distribution.8 Accuracy improves further on WIKI: extracted SQuAD models recover 95% of original accuracy despite seeing only nonsensical questions during training. While extracted models have high accuracy, their agreement is only slightly better than accuracy in most cases. Agreement is even lower on held-out sets constructed using the WIKI and RANDOM sampling scheme. On SQuAD, extracted WIKI and RANDOM have low agreements of 59.2 F1 and 50.5 F1 despite being trained on identically distributed data. This indicates poor functional equivalence between the victim and extracted model as also found by Jagielski et al. (2019). An ablation study with alternative query generation heuristics for SQuAD and mnli is conducted in Appendix A.4.
Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose universal language model fine-tuning (ulmfit), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100\u00d7 more data. We opensource our pretrained models and code 1 .
Recent works confirm that pre-trained models for phoneme representations, including png bert [16], mixed-phoneme bert [17] and phoneme-level bert [18], help improve advanced TTS systems. png bert and mixed-phoneme bert are trained based on the BERT pre-training approach [10], in which png bert takes both phonemes and graphemes (i.e. subword tokens) as the input, while mixed-phoneme bert takes both phonemes and sup-phoneme tokens as the input. phoneme-level bert is trained based on the ALBERT pretraining approach [12], only taking phonemes as the input. In addition to the standard masked token prediction task as used in png bert and mixed-phoneme bert, the phoneme-level bert also proposes an additional auxiliary task that predicts the corresponding grapheme for each phoneme. Here, png bert, mixed-phoneme bert and phoneme-level bert can be directly used as an input encoder in a typical neural TTS system. Note that the success of these pre-trained language models has been limited to the English language only. Taking into account a societal, linguistic, cultural, machine learning and cognitive perspective [19], it is worth exploring pre-trained models for phoneme representations in languages other than English.
Several classification algorithms are used to test gender information from iris texture images. Those algorithms are: Adaboost M1, LogitBoost, GentleBoost, RobustBoost, LP-Boost, TotalBoost and rusboost. Additionally, a Random Forest classifier with 500 trees, a Gini Index, and a LIB-SVM classifier with Gaussian Kernel (RBF) were also used. A comparison of the results obtained with these classifiers is shown in section 4.
Contributions Our contributions are the following: 1) We propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning for any task for NLP. 2) We propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. 3) We significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets. 4) We show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis. 5) We make the pretrained models and our code available to enable wider adoption.
On TREC-6, our improvement-similar as the improvements of state-of-the-art approaches-is not statistically significant, due to the small size of the 500-examples test set. Nevertheless, the competitive performance on TREC-6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences-in the case of TREC-6to several paragraphs for IMDb. Note that despite pretraining on more than two orders of magnitude less data than the 7 million sentence pairs used by McCann et al. (2017), we consistently outperform their approach on both datasets.
The cost of vision-and-language pre-training has become increasingly prohibitive due to end-toend training of large-scale models. This paper proposes BLIP-2, a generic and efficient pretraining strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. blip-2 bridges the modality gap with a lightweight Querying Transformer, which is pretrained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. blip-2 achieves state-of-the-art performance on various visionlanguage tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.
To this end, if not mentioned otherwise, we use the same set of hyperparameters across tasks, which we tune on the IMDb validation set. We use the awd-lstm language model (Merity et al., 2017a) with an embedding size of 400, 3 layers, 1150 hidden activations per layer, and a BPTT batch size of 70. We apply dropout of 0.4 to layers, 0.3 to RNN layers, 0.4 to input embedding layers, 0.05 to embedding layers, and weight dropout of 0.5 to the RNN hidden-to-hidden matrix. The classifier has a hidden layer of size 50.
We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al.,  2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction-membership classification and API watermarking-which while successful against naive adversaries, are ineffective against more sophisticated ones.
Several classification algorithms are used to test gender information from iris texture images. Those algorithms are: adaboost m1, logitboost, gentleboost, robustboost, lp-boost, totalboost and rusboost. Additionally, a random forest classifier with 500 trees, a gini index, and a lib-svm classifier with gaussian kernel (rbf) were also used. A comparison of the results obtained with these classifiers is shown in section 4.
BSIF have been used for several applications including biometrics from iris images [17,12,24]. In this work, a gender classification algorithm using normalised NIR iris images is proposed. It uses a similar pipeline than iris recognition systems. The iris is segmented and occlusions are masked. BSIF can be sensitive to image boundaries and the occlusion mask creating artificial texture which may mislead gender classification results.
Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examples (Brown et al., 2020). These few-shot properties first appeared when scaling models to a sufficient size (Kaplan et al., 2020), resulting in a line of work that focuses on further scaling these models (Chowdhery et al., 2022;Rae et al., 2021). These efforts are based on the assumption that more parameters will lead to better performance. However, recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data.
The fixed width of hidden vectors becomes a bottleneck, when the bidirectional lstm models must propagate dependencies over long distances over the questions and answers. An attention mechanism are used to alleviate this weakness by dynamically aligning the more informative parts of answers to the questions. This strategy has been used in many other natural language processing tasks, such as machine translation , sentence summarization  and factoid question answering. Inspired by the work , we develop a very simple but efficient word-level attention on the basic model. Figure 3 shows the structure. Prior to the average or mean pooling, each biLSTM output vector will be multiplied by a softmax weight, which is determined by the question embedding from biLSTM."
Do different victim models agree on the answers to nonsensical queries? We train five victim SQuAD models on the original training data with identical hyperparameters, varying only the random seed; each achieves an F1 between 90 and 90.5. Then, we measure the average pairwise F1 (\"agreement\") between the answers produced by these models for different types of queries. As expected, the models agree very frequently when queries come from the SQuAD training set (96.9 F1) or development set (90.4 F1). However, their agreement drops significantly on WIKI queries (53.0 F1) and even further on RANDOM queries (41.2 F1).9 Note that this result parallels prior work (Lakshminarayanan et al., 2017), where an ensemble of classifiers has been shown to provide better uncertainty estimates and out-of-distribution detection than a single overconfident classifier.
SQuAD dev set results comparing BERT-large and xlnet-large attacker architectures. Note the effectiveness of xlnet-large over BERT-large in both RANDOM and WIKI attack settings, despite seeing BERT-LARGE victim outputs during training. Legend: Training Data X, Y represent the input and output pairs used while training the attacker model; ORIGINAL represents the original SQuAD dataset; BERT-LARGE represents the outputs from the victim BERT-large model.
SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime 3 .
Our code and models will be released at https://gitlab.com/SysCV/SAM-HQ.
Mismatched architectures: BERT comes in two different sizes: the 24 layer BERT-large and the 12 layer BERT-base. In Table 4, we measure the development set accuracy on mnli and squad when the victim and attacker use different configurations of these two models. We notice that accuracy is always higher when the attacker starts from BERT-large, even when the victim was initialized with BERT-base. Additionally, given a fixed attacker architecture, accuracy is better when the victim uses the same model (e.g., if the attacker starts from BERT-base, they will have better results if the victim also used BERT-base). What if we train from scratch? Fine-tuning BERT or XLNet seems to give attackers a significant headstart, as only the final layer of the model is randomly initialized and the BERT parameters start from a good initialization representative of the properties of language. To measure the importance of fine-tuning from a good starting point, we train a QANet model (Yu et al., 2018) on squad with no contextualized pretraining. This model has 1.3 million randomly initialized parameters at the start of training. Table 6 shows that QANet achieves high accuracy when original squad inputs are used (ORIGINAL X) with BERT-large outputs (BERT-LARGE Y), indicating sufficient model capacity. However, the F1 significantly degrades when training on nonsensical RANDOM and WIKI queries. The F1 drop is particularly striking when compared to the corresponding rows in Table 2 (only 4.5 F1 drop for WIKI). This reinforces our finding that better pretraining allows models to start from a good representation of language, thus simplifying extraction.
Do different victim models agree on the answers to nonsensical queries? We train five victim squad models on the original training data with identical hyperparameters, varying only the random seed; each achieves an F1 between 90 and 90.5. Then, we measure the average pairwise F1 (\"agreement\") between the answers produced by these models for different types of queries. As expected, the models agree very frequently when queries come from the SQuAD training set (96.9 F1) or development set (90.4 F1). However, their agreement drops significantly on WIKI queries (53.0 F1) and even further on RANDOM queries (41.2 F1).9 Note that this result parallels prior work (Lakshminarayanan et al., 2017), where an ensemble of classifiers has been shown to provide better uncertainty estimates and out-of-distribution detection than a single overconfident classifier.
The iris is detected from the input image using commercial software osiris [23]. A segmentation mask occludes the eyelids, eyelashes and specular reflection portions of the iris image which are not useful for gender classification. It is important to note that iris images of different persons, or even the left and right iris images for a given person, may not present exactly the same mask and imaging conditions (see Figure 2). Illumination by LEDs during capture may come from either side of the sensor, specular highlights may be present in different places in the image. Eyelid and head position may also affect segmentation.
The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models, allowing for powerful zero-shot capabilities and flexible prompting.
Some methods freeze the language model to use the knowledge from LLMs for vision-to-language generation tasks (Tsimpoukelli et al., 2021;Alayrac et al., 2022;Chen et al., 2022a;Ma\u00f1as et al., 2023;Tiong et al., 2022;Guo et al., 2022).
SQuAD dev set results comparing bert-large and XLNet-large attacker architectures. Note the effectiveness of XLNet-large over BERT-large in both RANDOM and WIKI attack settings, despite seeing BERT-LARGE victim outputs during training. Legend: Training Data X, Y represent the input and output pairs used while training the attacker model; ORIGINAL represents the original SQuAD dataset; BERT-LARGE represents the outputs from the victim bert-large model.
On all datasets, fine-tuning the full model leads to the lowest error comparatively early in training, e.g. already after the first epoch on imdb. The error then increases as the model starts to overfit and knowledge captured through pretraining is lost. In contrast, ULMFiT is more stable and suffers from no such catastrophic forgetting; performance remains similar or improves until late epochs, which shows the positive effect of the learning rate schedule.
SQuAD dev set results comparing BERT-large and XLNet-large attacker architectures. Note the effectiveness of XLNet-large over BERT-large in both RANDOM and WIKI attack settings, despite seeing BERT-LARGE victim outputs during training. Legend: Training Data X, Y represent the input and output pairs used while training the attacker model; ORIGINAL represents the original SQuAD dataset ; BERT-LARGE represents the outputs from the victim BERT-large model.
Our careful design reuses and preserves the pre-trained model weights of SAM, while only introducing minimal additional parameters and computation.
We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techniques include adding more image transformations to the training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Vis ual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner.
Datasets and tasks We evaluate our method on six widely-studied datasets, with varying numbers of documents and varying document length, used by state-of-the-art text classification and transfer learning approaches (Johnson and Zhang, 2017;McCann et al., 2017) as instances of three common text classification tasks: sentiment analysis, question classification, and topic classification. We show the statistics for each dataset and task in Table 1.
With the recent success of contextualized pretrained representations for transfer learning, NLP models created by finetuning ELMo (Peters et al., 2018) and bert (Devlin et al., 2019) have become increasingly popular (Gardner et al., 2018). Contextualized pretrained representations boost performance and reduce sample complexity (Yogatama et al., 2019), and typically require only a shallow task-specific network-sometimes just a single layer as in bert. While these properties are advantageous for representation learning, we hypothesize that they also make model extraction easier.
The large-scale pre-trained language models, e.g. BERT [10], RoBERTa [11] and albert [12], have proved their effectiveness, improving state-of-the-art performances of various natural language processing research and application tasks. For TTS, some works incorporate contextualized word embeddings generated by the pre-trained BERT [10] into their standard encoder [13,14,15]. In general, an input phoneme sequence is fed into the standard TTS encoder to produce phoneme representations, while its corresponding raw text is fed into BERT to obtain contextualized word embeddings. To construct the input vectors of the TTS decoder, the produced representations of the input phonemes are concatenated with the BERT-based contextualized embedding of the corresponding word that the phonemes belong to. As a result, BERT helps increase the quality of the output synthesized speech. Here, the pre-trained BERT is used to provide additional contextual information for phoneme representations indirectly. Therefore, it might be better if the contextualized phoneme representations are directly produced by a pre-trained BERT-type model that is learned from unlabeled phoneme-level data.
Machine learning models represent valuable intellectual property: the process of gathering training data, iterating over model design, and tuning hyperparameters costs considerable money and effort. As such, these models are often only indirectly accessible through web APIs that allow users to query a model but not inspect its parameters. Malicious users might try to sidestep the expensive model development cycle by instead locally reproducing an existing model served by such an API. In these attacks, known as \"model stealing\" or \"model extraction\" (Lowd & Meek, 2005;Tram\u00e8r et al., 2016), the adversary issues a large number of queries and uses the collected (input, output) pairs to train a local copy of the model. Besides theft of intellectual property, extracted models may leak sensitive information about the training data (Tram\u00e8r et al., 2016) or be used to generate adversarial examples that evade the model served by the API (Papernot et al., 2017).
Most biometric recognition work pertaining to NIR iris images have focused on extracting the iris region from the captured ocular image (see Figure 1). Thus, algorithms for soft biometric prediction have typically focused on the iris region rather than the extended ocular region (see Figure 4). Recent work [28] based on the Binarized Statistical Image Feature (BSIF) descriptor has shown that the extended ocular region commonly imaged by iris recognition systems provides greater sex prediction accuracy than the iris-only region. Predicting soft biometric attributes from the ocular region provides one major advantage over the iris region in that it does not require a potentially error prone algorithm for iris region extraction. Bobeldyk and Ross [28] were able to achieve an 85.7% sex prediction accuracy using concatenated histograms from tesselated regions of the BSIF code computed from NIR ocular images.
In addition to performing the task of recognizing an individual [4], it is possible to predict attributes about the individual, such as gender, race and age, from the raw biometric data itself. These attributes are referred to as soft biometrics [5]. Soft biometric attributes may not be discriminative enough to uniquely identify an individual, but can be used to increase the recognition accuracy of a biometric system [6]. In addition to increased performance, there are several other motivating factors to glean these attributes from the raw biometric data.
The large-scale pre-trained language models, e.g. bert [10], Roberta [11] and ALbert [12], have proved their effectiveness, improving state-of-the-art performances of various natural language processing research and application tasks. For TTS, some works incorporate contextualized word embeddings generated by the pre-trained bert [10] into their standard encoder [13,14,15]. In general, an input phoneme sequence is fed into the standard TTS encoder to produce phoneme representations, while its corresponding raw text is fed into bert to obtain contextualized word embeddings. To construct the input vectors of the TTS decoder, the produced representations of the input phonemes are concatenated with the bert-based contextualized embedding of the corresponding word that the phonemes belong to. As a result, bert helps increase the quality of the output synthesized speech. Here, the pre-trained bert is used to provide additional contextual information for phoneme representations indirectly. Therefore, it might be better if the contextualized phoneme representations are directly produced by a pre-trained bert-type model that is learned from unlabeled phoneme-level data.
Tram\u00e8r et al. (2016)racy using WIKI queries on MNLI and SQuAD with mismatched BERT architectures between the victim and attacker.Note the trend: (large, large) > (base, large) > (base, base) > (large, base) where the (\u2022, \u2022) refers to (victim, attacker) pretraining.Next, we experiment with an alternative non-BERT pretrained language model as the attacker architecture.We use XLNet-large(Yang et al., 2019), which has been shown to outperform BERT-large in a large variety of downstream NLP tasks.In Table5, we compare XLNet-large and BERT-large attacker architectures keeping a fixed BERT-large victim architecture.Note the superior performance of xlnet-large attacker models on SQuAD compared to BERT-large in both RANDOM and WIKI attack settings, despite seeing a mismatched victim's (BERT-large) outputs during training.Our experiments are reminiscent of similar discussion inTram\u00e8r et al. (2016)on Occam Learning, or appropriate alignment of victim-attacker architectures. Overall, the results suggest that attackers can maximize their accuracy by fine-tuning more powerful language models, and that matching architectures is a secondary concern.
Recent works confirm that pre-trained models for phoneme representations, including png bert [16], mixed-phoneme bert [17] and Phoneme-level BERT [18], help improve advanced TTS systems. png bert and mixed-phoneme bert are trained based on the BERT pre-training approach [10], in which png bert takes both phonemes and graphemes (i.e. subword tokens) as the input, while mixed-phoneme bert takes both phonemes and sup-phoneme tokens as the input. Phoneme-level BERT is trained based on the ALBERT pretraining approach [12], only taking phonemes as the input. In addition to the standard masked token prediction task as used in png bert and mixed-phoneme bert, the Phoneme-level BERT also proposes an additional auxiliary task that predicts the corresponding grapheme for each phoneme. Here, png bert, mixed-phoneme bert and Phoneme-level BERT can be directly used as an input encoder in a typical neural TTS system. Note that the success of these pre-trained language models has been limited to the English language only. Taking into account a societal, linguistic, cultural, machine learning and cognitive perspective [19], it is worth exploring pre-trained models for phoneme representations in languages other than English.
We introduce llama, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, llama-13B outperforms GPT-3 (175B) on most benchmarks, and llama-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community 1 .
Gender classification based on iris images is promising despite challenging problems presented in terms of image analysis [20,36,30]. The human iris is an annular part between the pupil and the white sclera. The iris has an extraordinary structure and includes many interlacing minute features such as freckles, coronas, stripes, furrows, crypts and so on. These visible features, generally called the texture of the iris, are unique to each individual [1,10,11]. Research has also shown that the iris is essentially stable throughout a person's life. Furthermore, since the iris is externally visible, iris-based biometrics systems can be non-invasive to their users [10,11] which is important for practical applications. All these properties (i.e., uniqueness, stability and non-invasiveness) make gender classification suitable and attractive as a complement for achieving highly reliable personal identification.
The two extensions introduced previously are combined in a simple manner. First, the biLSTM hidden vectors of answers h a (t) are multiplied by s a,q (t), which is computed from the question average pooling vectors o q , and updated to h a (t), illustrated in Eq. 9-11. Then, the original question and updated answer hidden vectors serve as inputs of CNN structure respectively, such that the question context can be used to evaluate the softmax weights of the input of CNN. From the experiments, we observe that the two extensions vary on their contributions on the performance improvement according to different datasets. However, QA-LSTM/CNN with attention can outperform the baselines on both datasets.
We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, chinchilla-70b and palm-540b. We release all our models to the research community 1 .
Soft biometric information such as gender can contribute to many applications like as identification and security. This paper explores the use of a binary statistical features (BSIF) algorithm for classifying gender from iris texture images captured with NIR sensors. It uses the same pipeline for iris recognition systems consisting of iris segmentation, normalisation and then classification. Experiments show that applying BSIF is not straightforward since it can create artificial textures causing misclassification. In order to overcome this limitation, a new set of filters was trained from eye images and different sized filters with padding bands were tested on a subject-disjoint database. A Modified-BSIF (MBSIF) method was implemented. The latter achieved better gender classification results (94.6% and 91.33% for the left and right eye respectively). These results are competitive with the state of the art in gender classification. In an additional contribution, a novel gender labelled database was created and it will be available upon request.
Our multilingual pre-training dataset is constructed following three phases. The first phase is to collect text documents and then perform word and sentence segmentation as well as duplicate removal and text normalization. The second phase is to convert texts into phonemes, employing the charsiug2p toolkit [21] that supports 90+ languages and locales. Finally, the third phase is to perform phoneme segmentation.
Tram\u00e8r et al. (2016)racy using WIKI queries on MNLI and SQuAD with mismatched BERT architectures between the victim and attacker.Note the trend: (large, large) > (base, large) > (base, base) > (large, base) where the (\u2022, \u2022) refers to (victim, attacker) pretraining.Next, we experiment with an alternative non-BERT pretrained language model as the attacker architecture.We use XLNet-large(Yang et al., 2019), which has been shown to outperform BERT-large in a large variety of downstream NLP tasks.In Table5, we compare XLNet-large and bert-large attacker architectures keeping a fixed BERT-large victim architecture.Note the superior performance of XLNet-large attacker models on SQuAD compared to BERT-large in both RANDOM and WIKI attack settings, despite seeing a mismatched victim's (BERT-large) outputs during training.Our experiments are reminiscent of similar discussion inTram\u00e8r et al. (2016)on Occam Learning, or appropriate alignment of victim-attacker architectures. Overall, the results suggest that attackers can maximize their accuracy by fine-tuning more powerful language models, and that matching architectures is a secondary concern.
SQuAD dev set results comparing BERT-large and xlnet-large attacker architectures. Note the effectiveness of XLNet-large over BERT-large in both RANDOM and WIKI attack settings, despite seeing BERT-LARGE victim outputs during training. Legend: Training Data X, Y represent the input and output pairs used while training the attacker model; ORIGINAL represents the original SQuAD dataset; BERT-LARGE represents the outputs from the victim BERT-large model.
We study model extraction attacks against NLP APIs that serve bert-based models. These attacks are surprisingly effective at extracting good models with low query budgets, even when an attacker uses nonsensical input queries. Our results show that fine-tuning large pretrained language models simplifies the process of extraction for an attacker. Unfortunately, existing defenses against extraction, while effective in some scenarios, are generally inadequate, and further research is necessary to develop defenses robust in the face of adaptive adversaries who develop counter-attacks anticipating simple defenses. Other interesting future directions that follow from the results in this paper include (1) leveraging nonsensical inputs to improve model distillation on tasks for which it is difficult to procure input data; (2) diagnosing dataset complexity by using query efficiency as a proxy; and (3) further investigation of the agreement between victim models as a method to identify proximity in input distribution and its incorporation into an active learning setup for model extraction.
We evaluate the ability of our models to write code from a natural language description on two benchmarks: humaneval (Chen et al., 2021) and mbpp (Austin et al., 2021). For both tasks, the model receives a description of the program in a few sentences, as well as a few input-output examples. In humaneval, it also receives a function signature, and the prompt is formatted as natural code with the textual description and tests in a MATH +maj1@k GSM8k +maj1@k PaLM 8B docstring. The model needs to generate a Python program that fits the description and satisfies the test cases. In Table 8, we compare the pass@1 scores of our models with existing language models that have not been finetuned on code, namely PaLM and LaMDA (Thoppilan et al., 2022). PaLM and LLaMA were trained on datasets that contain a similar number of code tokens.
This section outlines the architecture and describes the multilingual pre-training corpus and optimization setup that we use for XPhoneBERT.
First, we evaluate our extraction procedure in a controlled setting where an attacker uses an identical number of queries as the original training dataset (Table 2); afterwards, we investigate different query budgets for each task (Table 3). We provide commercial cost estimates for these query budgets using the Google Cloud Platform's Natural Language API calculator. 7 We use two metrics for evaluation: Accuracy of the extracted models on the original development set, and Agreement between the outputs of the extracted model and the victim model on the original development set inputs. Note that these metrics are defined at a label level -metrics are calculated using the argmax labels of the probability vectors predicted by the victim and extracted model.
Another direction is to apply the method to novel tasks and models. While an extension to sequence labeling is straightforward, other tasks with more complex interactions such as entailment or question answering may require novel ways to pretrain and fine-tune. Finally, while we have provided a series of analyses and ablations, more studies are required to better understand what knowledge a pretrained language model captures, how this changes during fine-tuning, and what information different tasks require.
The cost of vision-and-language pre-training has become increasingly prohibitive due to end-toend training of large-scale models. This paper proposes blip-2, a generic and efficient pretraining strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pretrained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. blip-2 achieves state-of-the-art performance on various visionlanguage tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.
We present XPhoneBERT, the first multilingual model pretrained to learn phoneme representations for the downstream text-to-speech (TTS) task. Our XPhoneBERT has the same model architecture as bert-base, trained using the RoBERTa pre-training approach on 330M phoneme-level sentences from nearly 100 languages and locales. Experimental results show that employing XPhoneBERT as an input phoneme encoder significantly boosts the performance of a strong neural TTS model in terms of naturalness and prosody and also helps produce fairly high-quality speech with limited training data. We publicly release our pre-trained XPhoneBERT with the hope that it would facilitate future research and downstream TTS applications for multiple languages.
For each language whose locales do not have their own wikipedia data, 5 we randomly divide the language's wikipedia data into equal parts (each with the same number of sentences), with each part corresponding to a locale. For example, we divide 67 million English sentences into two equal parts that are then separately converted into phonemic descriptions in British English (eng-uk) and American English (eng-us).
Most VLP methods perform end-to-end pre-training using large-scale image-text pair datasets. As the model size keeps increasing, the pre-training can incur an extremely high computation cost. Moreover, it is inflexible for end-to-end pre-trained models to leverage readily-available unimodal pre-trained models, such as LLMs (Brown et al., 2020;Zhang et al., 2022;Chung et al., 2022).
Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.
Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.\nPreprint. Under review.
We also experiment with another setting where the tts training data is limited. In particular, for each language, we Table 2: Obtained results on the English test set. \"100%\" and \"5%\" denote the first experimental setting of using the whole tts training set and the second experimental setting of using only 5% of the TTS training set for training, respectively. \"XPB\" abbreviates our XPhoneBERT. The MOS is reported with 95% confidence intervals (here, each MOS score difference between two models is significant with p-value < 0.05). randomly sample 5% of the training audio clips, and then only use those sampled audios for training (total duration of about 1.2 hours for English and about 0.9 hours for Vietnamese). We apply the same training protocol used for the first setting with an exception that we run for 100K training steps.
Fine-tuning Fine-tuning has been used successfully to transfer between similar tasks, e.g. in QA (Min et al., 2017), for distantly supervised sentiment analysis (Severyn and Moschitti, 2015), or MT domains (Sennrich et al., 2015) but has been shown to fail between unrelated ones (Mou et al., 2016). Dai and Le (2015) also fine-tune a language model, but overfit with 10k labeled examples and require millions of in-domain documents for good performance. In contrast, ULMFiT leverages general-domain pretraining and novel finetuning techniques to prevent overfitting even with only 100 labeled examples and achieves state-ofthe-art results also on small datasets.
Several classification algorithms are used to test gender information from iris texture images. Those algorithms are: Adaboost M1, LogitBoost, GentleBoost, robustboost, LP-Boost, TotalBoost and RusBoost. Additionally, a Random Forest classifier with 500 trees, a Gini Index, and a LIB-SVM classifier with Gaussian Kernel (RBF) were also used. A comparison of the results obtained with these classifiers is shown in section 4.
To this end, if not mentioned otherwise, we use the same set of hyperparameters across tasks, which we tune on the imdb validation set. We use the AWD-LSTM language model (Merity et al., 2017a) with an embedding size of 400, 3 layers, 1150 hidden activations per layer, and a BPTT batch size of 70. We apply dropout of 0.4 to layers, 0.3 to RNN layers, 0.4 to input embedding layers, 0.05 to embedding layers, and weight dropout of 0.5 to the RNN hidden-to-hidden matrix. The classifier has a hidden layer of size 50.
Here, we employ the multilingual datasets wiki40b [22] and wikipedia [23], available to download from the Hugging Face datasets library [24]. In particular, we first download the wiki40b dataset consisting of text documents for 41 Wikipedia languages and locales. 1 We then use wikipedia to extract texts from Wikipedia dumps for remaining languages other than those belonging to wiki40b. 2 e perform word and sentence segmentation on all text documents in each language by using the spaCy toolkit, 3 except for Vietnamese where we employ RDRSegmenter [25] from the VnCoreNLP toolkit [26]. We then lowercase all sentences and filter out duplicate sentences and single-word ones. We also apply text normalization to convert texts from their written form into their verbalized form for only English, German, Spanish, Vietnamese and Chinese (it is because we could not find an effective text normalization tool publicly available for other languages). Here, we use the text normalization component from the nvidia nemo toolkit [27] for English, German, Spanish and Chinese, and the Vinorm text normalization package for Vietnamese.
We also experiment with another setting where the tts training data is limited. In particular, for each language, we Table 2: Obtained results on the English test set. \"100%\" and \"5%\" denote the first experimental setting of using the whole TTS training set and the second experimental setting of using only 5% of the tts training set for training, respectively. \"XPB\" abbreviates our XPhoneBERT. The MOS is reported with 95% confidence intervals (here, each MOS score difference between two models is significant with p-value < 0.05). randomly sample 5% of the training audio clips, and then only use those sampled audios for training (total duration of about 1.2 hours for English and about 0.9 hours for Vietnamese). We apply the same training protocol used for the first setting with an exception that we run for 100K training steps.
To fill the gap, we train the first large-scale multilingual language model for phoneme representations, using a pre-training corpus of 330M phonemic description sentences from nearly 100 languages and locales. Our model is trained based on the RoBERTa pre-training approach [11], using the BERT-base model configuration [10]. We conduct experiments on the downstream TTS task, directly employing our model as an input phoneme encoder of the strong model VITS [9]. Experimental results show that our model helps boost the performance of VITS, obtaining more natural prosody than the original VITS without pre-training and also producing fairly high-quality synthesized speech with limited training data. We summarize our contribution as follows:
We employ a white-space tokenizer, resulting in a vocabulary of 1960 phoneme types. Our XPhoneBERT thus has a total of 87.6M parameters. For training XPhoneBERT on our multilingual pre-training corpus, we employ the RoBERTa implementation [11] from the fairseq library [29]. We set a maximum sequence length of 512. We optimize the model using Adam [30] and use a batch size of 1024 sequence blocks across 8 A100 GPUs (40GB each) and a peak learning rate of 0.0001. We train for 20 epochs in about 18 days (here, the first 2 epochs are used for warming up the learning rate).
The fixed width of hidden vectors becomes a bottleneck, when the bidirectional LSTM models must propagate dependencies over long distances over the questions and answers. An attention mechanism are used to alleviate this weakness by dynamically aligning the more informative parts of answers to the questions. This strategy has been used in many other natural language processing tasks, such as machine translation , sentence summarization  and factoid question answering. Inspired by the work , we develop a very simple but efficient word-level attention on the basic model. Figure 3 shows the structure. Prior to the average or mean pooling, each biLSTM output vector will be multiplied by a softmax weight, which is determined by the question embedding from biLSTM."
SQuAD dev set results comparing BERT-large and XLNet-large attacker architectures. Note the effectiveness of XLNet-large over BERT-large in both RANDOM and WIKI attack settings, despite seeing bert-large victim outputs during training. Legend: Training Data X, Y represent the input and output pairs used while training the attacker model; ORIGINAL represents the original SQuAD dataset; BERT-LARGE represents the outputs from the victim BERT-large model.
SQuAD dev set results comparing BERT-large and XLNet-large attacker architectures. Note the effectiveness of xlnet-large over BERT-large in both RANDOM and WIKI attack settings, despite seeing BERT-LARGE victim outputs during training. Legend: Training Data X, Y represent the input and output pairs used while training the attacker model; ORIGINAL represents the original SQuAD dataset; BERT-LARGE represents the outputs from the victim BERT-large model.
Deep convolutional neural networks have recently been substantially improving upon the state of the art in image classification and other recognition tasks [2, 6,10]. Since their introduction in the early 1990s [7], convolutional neural networks have consistently been competitive with other techniques for image classification and recognition. Recently, they have pulled away from competing methods due the availability of larger data sets, better models and training algorithms and the availability of GPU computing to enable investigation of larger and deeper models.
For English, we use the benchmark dataset LJSpeech [32] consisting of 13,100 audio clips of a single speaker with a total duration of about 24 hours (here, each clip is also provided with a gold-standard text transcription). Following [9], the dataset is split into training, validation and test sets of 12,500, 100 and 500 clip samples, respectively. For Vietnamese, we randomly sample 12,300 different medium-length sentences from the PhoBERT pre-training news data [33]. We hire a professional speaker to read each sentence in a studio and record the corresponding audio, resulting in a total duration of about 18 hours for 12,300 high-quality audio clips. We split our vietnamese tts dataset into training, validation and test sets of 12,000, 100 and 200 clips, respectively.
In this work a gender classification method is proposed. It uses normalised iris texture information which is codified using mbsif. The outline of this paper is as follows: Section 2 reviews the state of the art in gender classification methods and describes the BSIF algorithm used in this work. Section 3 describes the pipeline of this work and the challenges faced when implementing mbsif algorithms. Experimental set-up and the results of gender classification using several classifiers and mbsif implementation settings are shown in Section 4. Finally, the conclusions are presented in section 5.
We also experiment with another setting where the TTS training data is limited. In particular, for each language, we Table 2: Obtained results on the English test set. \"100%\" and \"5%\" denote the first experimental setting of using the whole tts training set and the second experimental setting of using only 5% of the tts training set for training, respectively. \"XPB\" abbreviates our XPhoneBERT. The MOS is reported with 95% confidence intervals (here, each MOS score difference between two models is significant with p-value < 0.05). randomly sample 5% of the training audio clips, and then only use those sampled audios for training (total duration of about 1.2 hours for English and about 0.9 hours for Vietnamese). We apply the same training protocol used for the first setting with an exception that we run for 100K training steps.

So far we assumed that the victim and the attacker both fine-tune a pretrained BERT-large model. However, in practical scenarios, the attacker might not have information about the victim architecture. What happens when the attacker fine-tunes a different base model than the victim? What if the attacker extracts a QA model from scratch instead of fine-tuning a large pretrained language model? Here, we examine how much the extraction accuracy depends on the pretraining setup.
The cost of vision-and-language pre-training has become increasingly prohibitive due to end-toend training of large-scale models. This paper proposes blip-2, a generic and efficient pretraining strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. blip-2 bridges the modality gap with a lightweight Querying Transformer, which is pretrained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. blip-2 achieves state-of-the-art performance on various visionlanguage tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.
Whenever people log onto computers, access an ATM, pass through airport security, use credit cards, or enter highsecurity areas, their identities need to be verified [5,6]. There is tremendous interest in reliable and secure identification methods. An active research area of this involves gender classification. Algorithms for automatic gender classification have several applications. They can be used for database binning and retrieval, for intelligent user interfaces or visual surveillance. They can also be used to provide demographic information to improve social services, to facilitate payment methods and for marketing applications in general.
To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources. hq-sam is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs. We show the efficacy of hq-sam in a suite of 9 diverse segmentation datasets across different downstream tasks, where 7 out of them are evaluated in a zero-shot transfer protocol. Our code and models will be released at https://github.com/SysCV/SAM-HQ.
In this paper, we apply a general deep learning (DL) framework for the answer selection task, which does not depend on manually defined features or linguistic tools. The basic framework is to build the embeddings of questions and answers based on bidirectional long short-term memory (biLSTM) models, and measure their closeness by cosine similarity. We further extend this basic model in two directions. One direction is to define a more composite representation for questions and answers by combining convolutional neural network with the basic framework. The other direction is to utilize a simple but efficient attention mechanism in order to generate the answer representation according to the question context. Several variations of models are provided. The models are examined by two datasets, including TREC-QA and insuranceqa. Experimental results demonstrate that the proposed models substantially outperform several strong baselines.
We present the first large-scale pre-trained multilingual model for phoneme representations, which we name XPhoneBERT. \u2022 On the downstream TTS task, XPhoneBERT helps significantly improve the performance of the strong baseline vits, thus confirming its effectiveness.
Some methods freeze the image encoder, including the early work which adopts a frozen object detector to extract visual features (Chen et al., 2020;Li et al., 2020;Zhang et al., 2021), and the recent LiT (Zhai et al., 2022) which uses a frozen pre-trained image encoder for CLIP (Radford et al., 2021) pre-training.
Most biometric recognition work pertaining to NIR iris images have focused on extracting the iris region from the captured ocular image (see Figure 1). Thus, algorithms for soft biometric prediction have typically focused on the iris region rather than the extended ocular region (see Figure 4). Recent work [28] based on the binarized statistical image feature (bsif) descriptor has shown that the extended ocular region commonly imaged by iris recognition systems provides greater sex prediction accuracy than the iris-only region. Predicting soft biometric attributes from the ocular region provides one major advantage over the iris region in that it does not require a potentially error prone algorithm for iris region extraction. Bobeldyk and Ross [28] were able to achieve an 85.7% sex prediction accuracy using concatenated histograms from tesselated regions of the BSIF code computed from NIR ocular images.
To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods.
We also experiment with another setting where the tts training data is limited. In particular, for each language, we Table 2: Obtained results on the English test set. \"100%\" and \"5%\" denote the first experimental setting of using the whole tts training set and the second experimental setting of using only 5% of the tts training set for training, respectively. \"XPB\" abbreviates our XPhoneBERT. The MOS is reported with 95% confidence intervals (here, each MOS score difference between two models is significant with p-value < 0.05). randomly sample 5% of the training audio clips, and then only use those sampled audios for training (total duration of about 1.2 hours for English and about 0.9 hours for Vietnamese). We apply the same training protocol used for the first setting with an exception that we run for 100K training steps.
